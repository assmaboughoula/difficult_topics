1
00:00:00,008 --> 00:00:10,008
[SOUND].

2
00:00:11,025 --> 00:00:16,226
In this lecture, we're going to talk about
how to instantiate a vector space model,

3
00:00:16,226 --> 00:00:19,509
so that we can get a very
specific ranking function.

4
00:00:23,080 --> 00:00:27,740
So this is the, to continue
the discussion of the vector space model.

5
00:00:27,740 --> 00:00:32,800
Which is one particular approach
to design ranking function.

6
00:00:34,440 --> 00:00:38,547
And we are going to talk about how
we use the general framework of

7
00:00:38,547 --> 00:00:40,370
the the vector space model.

8
00:00:40,370 --> 00:00:48,270
As a guidance to instantiate the framework
to derive a specific ranking function.

9
00:00:48,270 --> 00:00:53,360
And we're going to cover the simplest
instantiation of the framework.

10
00:00:55,360 --> 00:00:58,390
So as we discussed in
the previous lecture.

11
00:00:58,390 --> 00:01:00,590
The vector space model
is really a framework.

12
00:01:00,590 --> 00:01:02,390
It isn't, didn't say.

13
00:01:05,460 --> 00:01:11,040
As we discussed in the previous lecture,
vector space model is really a framework.

14
00:01:11,040 --> 00:01:13,160
It doesn't, say many things.

15
00:01:14,700 --> 00:01:17,800
So for
example here it shows that it did not say

16
00:01:17,800 --> 00:01:19,470
how we should define the dimension.

17
00:01:20,770 --> 00:01:25,939
It also did not say how we place
a documented vector in this space.

18
00:01:27,130 --> 00:01:31,220
It did not say how we place a query
vector in this vector space.

19
00:01:32,500 --> 00:01:36,450
And finally, it did not say how
we should match a similarity

20
00:01:36,450 --> 00:01:39,020
between the query vector and
the document vector.

21
00:01:40,580 --> 00:01:44,870
So, you can imagine,
in order to implement this model.

22
00:01:46,040 --> 00:01:52,940
We have to see specifically,
how we are computing these vectors.

23
00:01:52,940 --> 00:01:56,700
What is exactly xi and what is exactly yi?

24
00:01:58,460 --> 00:02:02,260
This will determine where we
place the document vector.

25
00:02:02,260 --> 00:02:04,560
Where we place a query vector.

26
00:02:04,560 --> 00:02:08,869
And of course, we also need to say exactly
what will be the similarity function.

27
00:02:11,120 --> 00:02:15,449
So if we can provide a definition
of the concepts that would

28
00:02:15,449 --> 00:02:18,976
define the dimensions and
these xi's, or yi's.

29
00:02:18,976 --> 00:02:22,940
And then, the waits of terms for
query and document.

30
00:02:24,810 --> 00:02:28,683
Then we will be able to
place document vectors and

31
00:02:28,683 --> 00:02:33,080
query vector in this well defined space.

32
00:02:33,080 --> 00:02:36,313
And then,
if we also specify similarity function,

33
00:02:36,313 --> 00:02:39,699
then we'll have well
defined ranking function.

34
00:02:41,440 --> 00:02:43,790
So let's see how we can do that.

35
00:02:43,790 --> 00:02:47,630
And think about
the the simpliciter instantiation.

36
00:02:47,630 --> 00:02:52,460
Actually, I would suggest you to
pause the lecture at this point

37
00:02:52,460 --> 00:02:54,980
spend a couple of minute to think about.

38
00:02:54,980 --> 00:02:59,590
Suppose you are asked
to implement this idea.

39
00:02:59,590 --> 00:03:02,250
You've come up with the idea
of vector space model.

40
00:03:03,800 --> 00:03:07,687
But you still haven't figured out
how to compute this vector exactly,

41
00:03:07,687 --> 00:03:10,320
how to define this similarity function.

42
00:03:10,320 --> 00:03:10,820
What would you do?

43
00:03:12,550 --> 00:03:15,710
So think for a couple of minutes and
then, proceed.

44
00:03:20,720 --> 00:03:26,428
So let's think about some simplest ways
of instantiating this vector space model.

45
00:03:26,428 --> 00:03:28,810
First, how do we define a dimension.

46
00:03:28,810 --> 00:03:31,420
Well the obvious choice is we use

47
00:03:31,420 --> 00:03:35,410
each word in our vocabulary
to define a dimension.

48
00:03:35,410 --> 00:03:39,390
And a whole issue that there
are n words in our vocabulary,

49
00:03:39,390 --> 00:03:41,160
therefore there are n dimensions.

50
00:03:41,160 --> 00:03:42,822
Each word defines one dimension.

51
00:03:42,822 --> 00:03:46,971
And this is basically the Bag
of Words Instantiation.

52
00:03:48,876 --> 00:03:52,395
Now let's look at how we
place vectors in this space.

53
00:03:54,395 --> 00:03:57,175
Again here, the simplest of strategy is to

54
00:03:58,700 --> 00:04:03,550
use a bit vector to represent
both a query and a document.

55
00:04:04,700 --> 00:04:07,530
And that means each element xi and

56
00:04:07,530 --> 00:04:12,029
yi would be taking a value
of either zero or one.

57
00:04:13,270 --> 00:04:14,582
When it's one,

58
00:04:14,582 --> 00:04:20,499
it means the corresponding word is
present in the document or in the query.

59
00:04:20,499 --> 00:04:25,180
When it's zero,
it's going to mean that it's absent.

60
00:04:27,070 --> 00:04:31,220
So you can imagine if the user
types in a few word in your query.

61
00:04:31,220 --> 00:04:35,798
Then the query vector,
we only have a few ones, many, many zeros.

62
00:04:35,798 --> 00:04:41,450
The document vector in general
we have more ones of course,

63
00:04:41,450 --> 00:04:43,120
but we also have many zeros.

64
00:04:43,120 --> 00:04:46,700
So it seems the vocabulary
is generally very large.

65
00:04:46,700 --> 00:04:50,720
Many words don't really
occur in a document.

66
00:04:52,110 --> 00:04:56,884
Many words will only occasionally
occur in the document.

67
00:04:58,428 --> 00:05:02,178
A lot of words will be absent
in a particular document.

68
00:05:04,082 --> 00:05:09,760
So, now we have placed the documents and
the query in the vector space.

69
00:05:11,450 --> 00:05:14,240
Let's look at how we
match up the similarity.

70
00:05:15,780 --> 00:05:19,400
So, a commonly used similarity
measure here is Dot Product.

71
00:05:20,750 --> 00:05:25,590
The dot product of two
vectors is simply defined as

72
00:05:25,590 --> 00:05:30,580
the sum of the products of the
corresponding elements of the two vectors.

73
00:05:30,580 --> 00:05:39,031
So here we see that it's
the product of x1 and the y1.

74
00:05:39,031 --> 00:05:41,033
So here.

75
00:05:41,033 --> 00:05:44,043
And then, x2 multiplied by y2.

76
00:05:44,043 --> 00:05:47,100
And then finally xn multiplied by yn.

77
00:05:47,100 --> 00:05:48,780
And then we take a sum here.

78
00:05:50,630 --> 00:05:52,610
So that's the dot product.

79
00:05:52,610 --> 00:05:57,580
Now we can represent this in a more
general way, using a sum here.

80
00:05:58,740 --> 00:06:04,110
So this only one of the many different
ways of matching the similarity.

81
00:06:04,110 --> 00:06:10,610
So now we see that we have defined the,
the dimensions.

82
00:06:10,610 --> 00:06:13,160
We have defined the, the vectors.

83
00:06:13,160 --> 00:06:16,050
And we have also defined
the similarity function.

84
00:06:16,050 --> 00:06:19,490
So now we finally have
the Simplest Vector Space Model.

85
00:06:20,555 --> 00:06:24,289
Which is based on the bit vector
representation, dot product similarity,

86
00:06:24,289 --> 00:06:25,935
and bag of words instantiation.

87
00:06:27,035 --> 00:06:30,195
And the formula looks like this.

88
00:06:30,195 --> 00:06:32,415
So this is our formula.

89
00:06:32,415 --> 00:06:37,660
And that's actually a particular retrieval
function, a ranking function all right?

90
00:06:37,660 --> 00:06:42,930
Now, we can finally implement this
function using a program language and

91
00:06:42,930 --> 00:06:45,350
then rank documents for query.

92
00:06:45,350 --> 00:06:50,110
Now at this point you should
again pause the lecture

93
00:06:50,110 --> 00:06:53,400
to think about how we can
interpret this score.

94
00:06:53,400 --> 00:06:56,960
So we have gone through the process
of modeling the retrieval problem

95
00:06:58,930 --> 00:07:00,620
using a vector space model.

96
00:07:00,620 --> 00:07:02,229
And then, we make assumptions.

97
00:07:03,950 --> 00:07:09,780
About how we place vectors in the vector
space and how we define the similarity.

98
00:07:09,780 --> 00:07:14,250
So in the end we've got a specific
retrieval function shown here.

99
00:07:15,370 --> 00:07:18,340
Now the next step is to think about
what of this individual function

100
00:07:18,340 --> 00:07:19,600
actually makes sense?

101
00:07:20,610 --> 00:07:24,140
I, can we expect this function
to actually perform well?

102
00:07:24,140 --> 00:07:27,280
Where we use it to ramp it up,
for use in query.

103
00:07:28,790 --> 00:07:35,870
So, it's worth thinking about, what is
this value that we are calculating?

104
00:07:35,870 --> 00:07:40,250
So in the end, we've got a number,
but what does this number mean?

105
00:07:40,250 --> 00:07:40,990
Is it meaningful?

106
00:07:42,190 --> 00:07:44,340
So spend a couple minutes
to think about that.

107
00:07:45,880 --> 00:07:46,540
And of course,

108
00:07:46,540 --> 00:07:52,600
the general question here is do you
believe this is a good ranking function?

109
00:07:52,600 --> 00:07:54,500
Would it actually work well?

110
00:07:54,500 --> 00:07:58,510
So again,
think about how to interpret this value.

111
00:07:58,510 --> 00:08:00,200
Is it actually meaningful?

112
00:08:01,290 --> 00:08:03,210
Does it mean something?

113
00:08:03,210 --> 00:08:06,560
So related to how well that
document matches the query.

114
00:08:08,250 --> 00:08:11,530
So in order to assess
whether this simplest

115
00:08:11,530 --> 00:08:15,460
vector space model actually works well,
let's look at the example.

116
00:08:17,160 --> 00:08:22,570
So here I show some sample documents and
a simple query.

117
00:08:22,570 --> 00:08:26,730
The query is news about
the presidential campaign.

118
00:08:26,730 --> 00:08:28,580
And we have five documents here.

119
00:08:28,580 --> 00:08:32,220
They cover different, terms in the query.

120
00:08:34,720 --> 00:08:37,670
And if you look at the,
these documents for a moment.

121
00:08:38,880 --> 00:08:39,890
You may realize that

122
00:08:41,880 --> 00:08:46,910
some documents are probably relevant
in some cases or probably not relevant.

123
00:08:48,300 --> 00:08:52,698
Now if I ask you to rank these documents,
how would you rank them?

124
00:08:54,336 --> 00:08:56,160
This is basically our ideal ranking.

125
00:08:56,160 --> 00:08:57,270
Right.

126
00:08:57,270 --> 00:09:01,180
When humans can examine the documents and
then try to rank them.

127
00:09:03,450 --> 00:09:07,241
Now, so think for a moment and
take a look at this slide.

128
00:09:07,241 --> 00:09:10,190
And perhaps by pausing the lecture.

129
00:09:12,530 --> 00:09:16,090
So I think most of you
would agree that d4,

130
00:09:16,090 --> 00:09:19,620
and d3, are probably better than others.

131
00:09:19,620 --> 00:09:23,640
Because they really cover the query well.

132
00:09:23,640 --> 00:09:26,870
They match news,
presidential, and campaign.

133
00:09:27,900 --> 00:09:33,050
So, it looks like that these two documents
are probably better than the others.

134
00:09:33,050 --> 00:09:34,470
They should be ranked on top.

135
00:09:35,770 --> 00:09:41,800
And the other three, d1, d2, and
d5, are really non-relavant.

136
00:09:41,800 --> 00:09:46,480
So we can also say d4 and
d3 are relevent documents, and d1, d2, and

137
00:09:46,480 --> 00:09:48,525
d5 are non-relevant.

138
00:09:50,150 --> 00:09:55,440
So, now lets see if our vector
space model could do the same or

139
00:09:55,440 --> 00:09:57,400
could do something closer.

140
00:09:57,400 --> 00:10:02,260
So let's first think about how we actually
use this model to score documents.

141
00:10:03,780 --> 00:10:10,390
Right here I show two documents, d1 and
d3, and we have the query also here.

142
00:10:10,390 --> 00:10:15,130
In the vector space model, of course we
want to first compute the vectors for

143
00:10:15,130 --> 00:10:16,830
these documents and the query.

144
00:10:16,830 --> 00:10:19,240
Now I issue with the vocabulary
here as well, so

145
00:10:19,240 --> 00:10:22,850
these are the n dimensions
that we'll be thinking about.

146
00:10:22,850 --> 00:10:26,620
So what do you think is the vector
representation for the query?

147
00:10:27,700 --> 00:10:32,870
Note that we are assuming
that we only use zero and one

148
00:10:32,870 --> 00:10:39,230
to indicate whether a term is absent or
present in the query or in the document.

149
00:10:39,230 --> 00:10:42,380
So these are zero, one bit vectors.

150
00:10:43,880 --> 00:10:45,750
So what do you think is the query vector?

151
00:10:47,820 --> 00:10:51,200
Well the query has four words here.

152
00:10:51,200 --> 00:10:56,220
So for these four words, there would be a
one and for the rest, there will be zeros.

153
00:10:57,680 --> 00:10:59,290
Now what about the documents?

154
00:10:59,290 --> 00:11:00,570
It's the same.

155
00:11:00,570 --> 00:11:03,430
So d1 has two rows, news and about.

156
00:11:03,430 --> 00:11:06,218
So there are two ones here and
the rest are zeros.

157
00:11:06,218 --> 00:11:12,220
Similarly, so

158
00:11:12,220 --> 00:11:16,380
now that we have the two vectors,
let's compute the similarity.

159
00:11:17,470 --> 00:11:19,550
And we're going to use dot product.

160
00:11:19,550 --> 00:11:25,690
So you can see when we use dot product we
just, multiply the corresponding elements.

161
00:11:25,690 --> 00:11:26,380
Right.
So

162
00:11:26,380 --> 00:11:31,710
these two would be, form a,
be forming a product.

163
00:11:31,710 --> 00:11:33,950
And these two will
generate another product.

164
00:11:33,950 --> 00:11:37,410
And these two would generate yet
another product.

165
00:11:37,410 --> 00:11:38,210
And so on and so forth.

166
00:11:40,010 --> 00:11:42,259
Now you can,
you need to see if we do that.

167
00:11:44,470 --> 00:11:48,860
We actually don't have to
care about these zeroes

168
00:11:50,380 --> 00:11:54,140
because if whenever we have a zero,
the product will be zero.

169
00:11:54,140 --> 00:11:57,530
So, when we take a sum
over all these pairs,

170
00:11:59,170 --> 00:12:02,940
then the zero entries will be gone.

171
00:12:04,400 --> 00:12:08,240
As long as you have one zero,
then the product would be zero.

172
00:12:08,240 --> 00:12:15,090
So in the fact, we're just counting
how many pairs of one and one, right?

173
00:12:15,090 --> 00:12:16,980
In this case, we have seen two.

174
00:12:16,980 --> 00:12:18,220
So the result will be two.

175
00:12:18,220 --> 00:12:20,240
So, what does that mean?

176
00:12:20,240 --> 00:12:25,190
Well that means, this number or
the value of this scoring function.

177
00:12:25,190 --> 00:12:30,260
Is simply the count of how many unique
query terms are matched in the document.

178
00:12:32,050 --> 00:12:36,920
Because if a document,
if a term is matched in the document,

179
00:12:36,920 --> 00:12:39,325
then there will be two ones.

180
00:12:41,390 --> 00:12:44,740
If it's not, then there will
be zero on the document side.

181
00:12:46,310 --> 00:12:49,200
Similarly, if the document has a term,.

182
00:12:49,200 --> 00:12:53,210
But the terms not in the query there
will be zero in the query vector.

183
00:12:53,210 --> 00:12:55,020
So those don't count.

184
00:12:55,020 --> 00:12:58,740
So as a result this
scoring function basically

185
00:12:58,740 --> 00:13:03,820
meshes how many unique query
terms are matched in a document.

186
00:13:03,820 --> 00:13:05,770
This is how we interpret this score.

187
00:13:07,150 --> 00:13:10,520
Now we can also take a look at the d3.

188
00:13:10,520 --> 00:13:15,770
In this case,
you can see the result is three.

189
00:13:15,770 --> 00:13:21,640
Because d3 matched the three distinctive
query words, news, presidential, campaign.

190
00:13:21,640 --> 00:13:23,140
Whereas d1 only matched two.

191
00:13:23,140 --> 00:13:28,210
Now in this case, it seems
reasonable to rank d3 on top of d1.

192
00:13:29,260 --> 00:13:33,440
And this simplest vector
space model indeed does that.

193
00:13:33,440 --> 00:13:35,050
So that looks pretty good.

194
00:13:35,050 --> 00:13:40,030
However, if we examine this model in
detail, we likely will find some problems.

195
00:13:40,030 --> 00:13:45,220
So here I'm going to show all
the scores for these five documents.

196
00:13:46,620 --> 00:13:49,020
And you can even verify they are correct.

197
00:13:49,020 --> 00:13:51,690
Because we're basically counting

198
00:13:51,690 --> 00:13:55,110
the number of unique query
terms matched in each document.

199
00:13:56,470 --> 00:13:58,910
Now note that this method
actually makes sense.

200
00:13:58,910 --> 00:14:02,050
Right?
It basically means if a document matches

201
00:14:02,050 --> 00:14:07,210
more unique query terms, then the document
will be assuming to be more relevant.

202
00:14:07,210 --> 00:14:09,180
And that seems to make sense.

203
00:14:09,180 --> 00:14:16,870
The only problem is here, we can note set
there are three documents, d2, d3, and d4.

204
00:14:16,870 --> 00:14:22,290
And they tied with a three, as a score.

205
00:14:25,050 --> 00:14:29,700
So that's a problem, because if you
look at them carefully it seems that

206
00:14:30,970 --> 00:14:35,860
d4 should be right above d3.

207
00:14:35,860 --> 00:14:39,640
Because d3 only mentioned
the presidential once.

208
00:14:39,640 --> 00:14:42,100
But d4 mentioned it much more times.

209
00:14:42,100 --> 00:14:47,365
In case of d3,
presidential could be extended mentioned.

210
00:14:47,365 --> 00:14:51,126
But d4 is clearly above
presidential campaign.

211
00:14:51,126 --> 00:14:58,200
Another problem is that d2 and
d3 also have the same soul.

212
00:14:58,200 --> 00:15:01,200
But, if you look at the,
the three words that are matched.

213
00:15:01,200 --> 00:15:07,020
In the case of d2, it matched the news,
about, and the campaign.

214
00:15:07,020 --> 00:15:11,500
But in the case of d3, it match the news,
presidential, and campaign.

215
00:15:12,530 --> 00:15:15,460
So intuitively, d3 is better.

216
00:15:15,460 --> 00:15:21,910
Because matching presidential is more
important though than matching about.

217
00:15:21,910 --> 00:15:24,920
Even though about and
the presidential are both in the query.

218
00:15:26,170 --> 00:15:29,650
So intuitively,
we would like d3 to be ranked above d2.

219
00:15:30,730 --> 00:15:32,760
But this model, doesn't do that.

220
00:15:33,860 --> 00:15:39,160
So that means this is still not good
enough, we have to solve these problems.

221
00:15:41,340 --> 00:15:42,220
To summarize,

222
00:15:42,220 --> 00:15:45,720
in this lecture we talked about how
to instantiate a vector space model.

223
00:15:47,630 --> 00:15:49,540
We may need to do three things.

224
00:15:49,540 --> 00:15:51,890
One is to define the dimension.

225
00:15:51,890 --> 00:15:55,880
The second is to

226
00:15:55,880 --> 00:16:01,670
decide how to place documents
as vectors in the vector space.

227
00:16:01,670 --> 00:16:05,650
And to also place a query in
the vector space as a vector.

228
00:16:08,090 --> 00:16:13,910
And third is to define
the similarity between two vectors,

229
00:16:13,910 --> 00:16:15,740
particularly the query vector and
the document vector.

230
00:16:17,080 --> 00:16:22,430
We also talked about a very simple way
to instantiate the vector space model.

231
00:16:22,430 --> 00:16:27,920
Indeed, that's probably the simplest
vector space model that we can derive.

232
00:16:27,920 --> 00:16:31,480
In this case,
we use each word to define a dimension.

233
00:16:31,480 --> 00:16:37,430
We use a zero one bit vector to
represent a document or a query.

234
00:16:37,430 --> 00:16:42,690
In this case, we basically only care
about word presence or absence.

235
00:16:42,690 --> 00:16:43,780
We ignore the frequency.

236
00:16:45,560 --> 00:16:49,220
And we use the dot product
as the similarity function.

237
00:16:50,360 --> 00:16:53,100
And with such a, a, in situation.

238
00:16:53,100 --> 00:16:58,870
And we showed that the scoring
function is basically to score

239
00:16:58,870 --> 00:17:03,250
a document based on the number of distinct
query words matched in the document.

240
00:17:04,640 --> 00:17:09,600
We also show that such a single vector
space model still doesn't work well,

241
00:17:09,600 --> 00:17:10,790
and we need to improve it.

242
00:17:12,540 --> 00:17:18,741
And this is the topic that we're
going to cover in the next lecture.

243
00:17:18,741 --> 00:17:28,741
[MUSIC]

