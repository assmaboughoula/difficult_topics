1
00:00:00,012 --> 00:00:04,379
[NOISE].

2
00:00:06,701 --> 00:00:10,014
There are many more advanced learning
algorithms than the regression based

3
00:00:10,014 --> 00:00:11,300
reproaches.

4
00:00:11,300 --> 00:00:13,860
And they generally
account to theoretically

5
00:00:13,860 --> 00:00:14,977
optimize or retrieval method.

6
00:00:16,690 --> 00:00:18,010
Like map or nDCG.

7
00:00:19,020 --> 00:00:24,630
Note that the optimization objecting
function that we have seen

8
00:00:24,630 --> 00:00:29,601
on the previous slide is not directly
related to retrieval measure.

9
00:00:31,220 --> 00:00:31,877
Right?

10
00:00:31,877 --> 00:00:34,930
By maximizing the prediction of one or
zero.

11
00:00:34,930 --> 00:00:39,430
Or we don't necessarily optimize
the ranking of those documents.

12
00:00:39,430 --> 00:00:44,210
One can imagine that why,
our prediction may not be too bad and

13
00:00:44,210 --> 00:00:45,880
let's say both are around 0.5.

14
00:00:45,880 --> 00:00:49,400
So it's kind of in the middle of zero and
one for

15
00:00:49,400 --> 00:00:53,000
the two documents, but
the ranking can be wrong.

16
00:00:53,000 --> 00:00:56,590
So we might have the, a larger value for.

17
00:00:56,590 --> 00:00:58,120
D2 and then e1.

18
00:01:00,750 --> 00:01:03,970
So that won't be good from
retrieval perspective,

19
00:01:03,970 --> 00:01:07,430
even though by likelihood function,
it's not bad.

20
00:01:07,430 --> 00:01:10,790
In contrast, we might have another
case where we predicted values.

21
00:01:10,790 --> 00:01:14,000
Or around 0.9 let's say,

22
00:01:14,000 --> 00:01:17,980
and by the objective function,
the error will be larger, but if we

23
00:01:17,980 --> 00:01:22,950
can get the order of the two documents
correct, that's actually a better result.

24
00:01:22,950 --> 00:01:28,070
So these new more advanced approaches
will try to correct that problem.

25
00:01:28,070 --> 00:01:30,510
Of course then the challenge is that.

26
00:01:30,510 --> 00:01:33,690
That the optimization problem
will be harder to solve.

27
00:01:33,690 --> 00:01:37,700
And then researchers have proposed
many solutions to the problem.

28
00:01:37,700 --> 00:01:42,100
And you can read more of
the references at the end.

29
00:01:42,100 --> 00:01:46,570
Know more about the these approaches.

30
00:01:46,570 --> 00:01:49,500
Now these learning to random approaches.

31
00:01:49,500 --> 00:01:53,530
Are actually general, so they can also be
applied to many other ranking problems,

32
00:01:53,530 --> 00:01:55,350
not just retrieval problem.

33
00:01:55,350 --> 00:01:58,999
So here I list some for
example recommender systems,

34
00:01:58,999 --> 00:02:03,136
computational adv, advertising,
or summarization, and

35
00:02:03,136 --> 00:02:08,913
there are many others that you can
probably encounter in your applications.

36
00:02:11,294 --> 00:02:16,174
To summarize this lecture,
we have talked about, using machine

37
00:02:16,174 --> 00:02:21,060
learning to combine much more features
to incorporate a ranking without.

38
00:02:22,780 --> 00:02:24,690
Actually the use of machine learning,

39
00:02:25,810 --> 00:02:29,590
in information retrieval has
started since many decades ago.

40
00:02:29,590 --> 00:02:35,546
So for example on the Rocchio feedback
approach that we talked about earlier

41
00:02:35,546 --> 00:02:40,801
was a machine learning approach
applied to to learn this feedback, but

42
00:02:40,801 --> 00:02:47,500
the most reasonable use of machine
learning has been driven by some changes.

43
00:02:47,500 --> 00:02:51,690
In the environment of applications
of retrieval systems.

44
00:02:51,690 --> 00:02:54,420
And first it's, mostly,

45
00:02:55,450 --> 00:03:00,090
driven by the availability of a lot of
training data in the form of clicks rules.

46
00:03:01,330 --> 00:03:04,250
Such data weren't available before.

47
00:03:04,250 --> 00:03:10,670
So the data can provide a lot
of useful knowledge about

48
00:03:10,670 --> 00:03:15,790
relevance and machine learning methods
can be applied to leverage this.

49
00:03:17,920 --> 00:03:20,800
Secondly it's also due by
the need of combining them.

50
00:03:20,800 --> 00:03:21,380
In the features.
And

51
00:03:21,380 --> 00:03:26,530
this is not only just because there
are more features available on the web

52
00:03:26,530 --> 00:03:29,840
that can be naturally re-used
with improved scoring.

53
00:03:29,840 --> 00:03:35,900
It's also because by combining them,
we can improve the robustness of ranking.

54
00:03:35,900 --> 00:03:39,839
So this is designed for combating spams.

55
00:03:41,456 --> 00:03:45,770
Modern search engines all use some kind
of machine learning techniques to combine

56
00:03:45,770 --> 00:03:48,830
many features to optimize ranking and

57
00:03:48,830 --> 00:03:53,580
this is a major feature of these
current engines such as Google, Bing.

58
00:03:56,478 --> 00:03:59,400
The topic of learning to rank
is still active research.

59
00:03:59,400 --> 00:04:04,640
Topic in the community, and so you can
expect to see new results being developed,

60
00:04:06,320 --> 00:04:08,790
in the next, few years.

61
00:04:08,790 --> 00:04:09,934
Perhaps.

62
00:04:12,779 --> 00:04:16,692
Here are some additional readings that
can give you more information about.

63
00:04:16,692 --> 00:04:21,278
About, how learning to rank books and

64
00:04:21,278 --> 00:04:25,271
also some advanced methods.

65
00:04:25,271 --> 00:04:35,271
[MUSIC]

