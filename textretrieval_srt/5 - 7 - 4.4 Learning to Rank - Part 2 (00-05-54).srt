1
00:00:00,025 --> 00:00:08,649
[SOUND] So
now let's take a look at the specific,

2
00:00:08,649 --> 00:00:14,921
method that's based on regression.

3
00:00:14,921 --> 00:00:17,564
Now this is one of the many
different methods in fact,

4
00:00:17,564 --> 00:00:19,940
it's the one of the simplest methods.

5
00:00:19,940 --> 00:00:24,550
And I choose this to explain the idea
because it's it's so simple.

6
00:00:26,360 --> 00:00:34,330
So in this approach we simply assume
that the relevance of a document

7
00:00:34,330 --> 00:00:39,730
with respect to the query, is related to
a linear combination of all the features.

8
00:00:39,730 --> 00:00:47,390
Here I used the Xi to emote the feature.

9
00:00:47,390 --> 00:00:51,760
So Xi of Q and D is a feature.

10
00:00:51,760 --> 00:00:54,720
And we can have as many features as,
we would like.

11
00:00:55,890 --> 00:01:01,710
And we assume that these features
can be combined in a linear manner.

12
00:01:03,580 --> 00:01:06,140
And each feature is controlled
by a parameter here.

13
00:01:06,140 --> 00:01:10,790
And this beta is a parameter,
that's a weighting parameter.

14
00:01:10,790 --> 00:01:16,080
A larger value would mean the feature
would have a higher weight and

15
00:01:16,080 --> 00:01:18,580
it would contribute more
to the scoring function.

16
00:01:18,580 --> 00:01:23,620
The specific form of the function
actually also involves

17
00:01:23,620 --> 00:01:27,370
a transformation of
the probability of relevance.

18
00:01:27,370 --> 00:01:29,399
So this is the probability of relevance.

19
00:01:30,740 --> 00:01:36,350
We know that the probability of relevance
is within the range from 0 to 1.

20
00:01:36,350 --> 00:01:40,840
And we could have just assumed
that the scoring function is

21
00:01:40,840 --> 00:01:43,860
related to this linear combination.

22
00:01:43,860 --> 00:01:47,850
Right, so we can do a,
a linear regression but

23
00:01:47,850 --> 00:01:53,900
then the value of this linear
combination could easily go beyond 1.

24
00:01:53,900 --> 00:01:58,758
So this transformation here would map ze,

25
00:01:58,758 --> 00:02:05,706
0 to 1 range through the whole
range of real values.

26
00:02:05,706 --> 00:02:08,330
You can, you can verify it,
it by yourself.

27
00:02:10,350 --> 00:02:17,603
So this allows us then to connect
to the probability of relevance

28
00:02:17,603 --> 00:02:23,000
which is between 0 and 1 to a linear
combination of arbitrary efficients.

29
00:02:23,000 --> 00:02:28,420
And if we rewrite this into a probability
function, we will get the next one.

30
00:02:28,420 --> 00:02:34,299
So on this side on this equation,
we will have the probability of relevance.

31
00:02:35,460 --> 00:02:38,230
And on the right hand side,
we will have this form.

32
00:02:39,460 --> 00:02:42,620
Now this form is created non-active.

33
00:02:42,620 --> 00:02:46,570
And it still involves the linear
combination of features.

34
00:02:46,570 --> 00:02:51,638
And it's also clear that is,
if this value is,

35
00:02:51,638 --> 00:02:54,581
is.

36
00:02:54,581 --> 00:02:59,250
Of the linear combination
in the equation above.

37
00:02:59,250 --> 00:03:03,310
If this this, this value here,

38
00:03:05,780 --> 00:03:12,190
if this value is large then it
will mean this value is small.

39
00:03:12,190 --> 00:03:16,920
And therefore, this probability,
this whole probability, would be large.

40
00:03:16,920 --> 00:03:18,530
And that's what we expect.

41
00:03:18,530 --> 00:03:22,950
Basically, it would be if this
combination gives us a high value,

42
00:03:22,950 --> 00:03:25,234
then the document's more likely relevant.

43
00:03:26,715 --> 00:03:28,995
So this is our hypothesis.

44
00:03:28,995 --> 00:03:32,545
Again, this is not necessarily
the best hypothesis.

45
00:03:32,545 --> 00:03:35,215
That this is a simple way to connect

46
00:03:35,215 --> 00:03:39,109
these features with
the probability of relevance.

47
00:03:40,500 --> 00:03:44,200
So now we have this this
combination function.

48
00:03:44,200 --> 00:03:49,440
The next task is to see how we
need to estimate the parameters so

49
00:03:49,440 --> 00:03:52,670
that the function can truly be applied.

50
00:03:52,670 --> 00:03:53,590
Right.
Without them knowing

51
00:03:53,590 --> 00:03:58,520
that they have values, it's,
it's harder to apply this function, okay.

52
00:03:58,520 --> 00:04:01,961
So let's how we can estimate, beta values.

53
00:04:04,214 --> 00:04:07,230
All right.
Let's take a look, at a simple example.

54
00:04:08,780 --> 00:04:11,490
In this example, we have three features.

55
00:04:11,490 --> 00:04:15,070
One is BM25 score of
the document under the query.

56
00:04:15,070 --> 00:04:19,060
One is the page rank score of
the document, which might or

57
00:04:19,060 --> 00:04:20,770
might not depend on the query.

58
00:04:20,770 --> 00:04:24,660
Hm, we might have a top
sensitive page rank.

59
00:04:24,660 --> 00:04:25,930
That would depend on the query.

60
00:04:25,930 --> 00:04:30,980
Otherwise, the general page rank
doesn't really depend on the query.

61
00:04:30,980 --> 00:04:34,410
And then we have BM25 score on
the Anchor task of the document.

62
00:04:35,670 --> 00:04:40,720
These are then the feature values for
a particular doc, document query pair.

63
00:04:41,900 --> 00:04:44,870
And in this case the document is D1.

64
00:04:44,870 --> 00:04:47,210
And the,
the judgment says that it's relevant.

65
00:04:47,210 --> 00:04:52,300
Here's another training instance,
and these features values.

66
00:04:54,860 --> 00:04:58,035
But in this case it's non-relevant, okay?

67
00:04:58,035 --> 00:05:02,565
This is a overly simplified case,
where we just have two instances.

68
00:05:03,665 --> 00:05:06,675
But it,
it's sufficient to illustrate the point.

69
00:05:06,675 --> 00:05:11,057
So what we can do is we use the maximum
likelihood estimator to actually estimate

70
00:05:11,057 --> 00:05:13,170
the parameters.

71
00:05:13,170 --> 00:05:18,921
Basically, we're going to do, predict
the relevance status of the document,

72
00:05:18,921 --> 00:05:22,040
the, based on the feature values.

73
00:05:22,040 --> 00:05:25,420
That is given that we observe
these feature values here.

74
00:05:28,490 --> 00:05:32,000
Can we predict the relevance?

75
00:05:32,000 --> 00:05:32,980
Yeah.

76
00:05:32,980 --> 00:05:39,070
And of course, the prediction will be
using this function that you see here.

77
00:05:39,070 --> 00:05:42,680
And we hypothesize this that
the probability of relevance is related

78
00:05:42,680 --> 00:05:43,920
features in this way.

79
00:05:43,920 --> 00:05:46,570
So we're going to see for

80
00:05:46,570 --> 00:05:51,510
what values of beta we can
predict that the relevance well.

81
00:05:51,510 --> 00:05:52,440
What do we mean?

82
00:05:52,440 --> 00:05:58,520
Well, what, what do we mean by
predicting the relevance well?

83
00:05:58,520 --> 00:05:59,770
Well we just mean.

84
00:05:59,770 --> 00:06:03,960
In the first case for D1,
this expression here,

85
00:06:03,960 --> 00:06:06,960
right here, should give higher values.

86
00:06:06,960 --> 00:06:10,810
In fact, they would hope this
to give a value close to one.

87
00:06:10,810 --> 00:06:14,678
Why?
Because this is a relevant document.

88
00:06:14,678 --> 00:06:21,580
On the other hand, in the second case for
D2 we hope this value would be small.

89
00:06:21,580 --> 00:06:22,310
Right.

90
00:06:22,310 --> 00:06:25,230
Why?
It's because it's a non-relevant document.

91
00:06:26,580 --> 00:06:30,250
So now let's see how this can
be mathematical expressed.

92
00:06:30,250 --> 00:06:35,640
And this is similar to,
expressing the probability of a document.

93
00:06:35,640 --> 00:06:38,930
Only that we are not talking about
the probability of words but

94
00:06:38,930 --> 00:06:41,680
talking about the probability
of relevance, 1 or 0.

95
00:06:41,680 --> 00:06:46,780
So what's the probability
of this document?

96
00:06:48,420 --> 00:06:52,870
The relevant if it has
these feature values.

97
00:06:54,250 --> 00:06:55,770
Well this is.

98
00:06:55,770 --> 00:06:57,890
Just this expression, right?

99
00:06:57,890 --> 00:07:00,970
We just need to pluck in the X, the Xis.

100
00:07:00,970 --> 00:07:02,166
So that's what we'll get.

101
00:07:02,166 --> 00:07:07,542
It's exactly like, what we have seen that,

102
00:07:07,542 --> 00:07:11,490
only that we replace these Xis.

103
00:07:11,490 --> 00:07:14,780
With now specific values.

104
00:07:14,780 --> 00:07:20,910
And so, for example, this 0.7 goes
to here and this 0.11 goes to here.

105
00:07:20,910 --> 00:07:28,200
And these are different feature values and
we'll combine them in this particular way.

106
00:07:28,200 --> 00:07:31,750
The beta values are still unknown.

107
00:07:31,750 --> 00:07:34,410
But this gives us the probability

108
00:07:34,410 --> 00:07:39,650
that this document is relevant
if we assume such a model.

109
00:07:39,650 --> 00:07:42,420
Okay, and
we want to maximize this probability since

110
00:07:42,420 --> 00:07:43,770
this is a random document.

111
00:07:44,880 --> 00:07:48,140
What we do for the second document.

112
00:07:48,140 --> 00:07:52,090
Well, we want to compute to the
probability that the predictions is, is n,

113
00:07:52,090 --> 00:07:53,630
non-relevant.

114
00:07:53,630 --> 00:08:02,290
So, this would mean, we have to compute
a 1 minus, right this expression.

115
00:08:02,290 --> 00:08:04,190
Since this expression.

116
00:08:05,400 --> 00:08:10,490
Is actually the probability of relevance,
so to compute the non relevance

117
00:08:10,490 --> 00:08:18,480
from relevance, we just do 1 minus
the probability of relevance, okay?

118
00:08:18,480 --> 00:08:20,730
So this whole expression then.

119
00:08:20,730 --> 00:08:29,210
Just is our probability of predicting
these two relevance values.

120
00:08:29,210 --> 00:08:30,590
One is 1.

121
00:08:30,590 --> 00:08:31,940
Here, one is a 0.

122
00:08:31,940 --> 00:08:37,770
And this whole equation
is our probability.

123
00:08:37,770 --> 00:08:42,570
Of observing a 1 here and
observing a 0 here.

124
00:08:44,090 --> 00:08:50,130
Of course this probability depends
on the beta values, right?

125
00:08:50,130 --> 00:08:52,840
So then our goal is to

126
00:08:52,840 --> 00:08:57,840
adjust the beta values to make this
whole thing reach its maximum.

127
00:08:57,840 --> 00:08:59,270
Make that as large as possible.

128
00:09:00,460 --> 00:09:02,540
So that means we
are going to compute this.

129
00:09:02,540 --> 00:09:08,220
The beta is just the, the parameter
values that would maximize this for

130
00:09:08,220 --> 00:09:10,290
like holder expression.

131
00:09:12,210 --> 00:09:16,260
And what it means is if
look at the function is

132
00:09:16,260 --> 00:09:20,290
we're going to choose betas to
make this as large as possible.

133
00:09:20,290 --> 00:09:25,020
And make this also as large as possible

134
00:09:25,020 --> 00:09:29,390
which is equivalent to say make
this the part as small as possible.

135
00:09:30,560 --> 00:09:32,360
And this is precisely what we want.

136
00:09:34,500 --> 00:09:39,060
So once we do the training,
now we will know the beta values.

137
00:09:39,060 --> 00:09:45,716
So then this function will be well
defined once their values are known.

138
00:09:45,716 --> 00:09:50,690
Both this and
this will become pretty less specified.

139
00:09:50,690 --> 00:09:55,857
So for any new query and new document we
can simply compute the features [NOISE]

140
00:09:55,857 --> 00:10:00,961
For that pair and then we just use this
formula to generate a ranking score.

141
00:10:00,961 --> 00:10:06,670
And this scoring function can be used in
for rank documents for a particular query.

142
00:10:06,670 --> 00:10:10,211
So that's the basic idea of,
learning to rank.

143
00:10:11,711 --> 00:10:21,711
[MUSIC]

