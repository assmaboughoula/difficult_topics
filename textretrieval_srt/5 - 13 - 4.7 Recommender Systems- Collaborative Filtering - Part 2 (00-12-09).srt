1
00:00:00,000 --> 00:00:08,711
[NOISE].

2
00:00:08,711 --> 00:00:14,075
And here what will do is
talk about basic strategy,

3
00:00:14,075 --> 00:00:19,688
and that would be based on
similarity of users and

4
00:00:19,688 --> 00:00:24,801
then predicting the rating
of an object by a, a,

5
00:00:24,801 --> 00:00:32,540
active user using the ratings of
similar users to this active user.

6
00:00:32,540 --> 00:00:38,620
This is called a memory-based approach
because it's a little bit similar to

7
00:00:40,110 --> 00:00:44,330
storing all the user information.

8
00:00:44,330 --> 00:00:49,820
And when we are considering a particular
user, we're going to try to

9
00:00:49,820 --> 00:00:56,300
kind of retrieve the relevant users, or
the similar users through this user case.

10
00:00:56,300 --> 00:00:57,630
And then try to use that

11
00:00:58,880 --> 00:01:03,010
user's information about those users
to predict the preference of this user.

12
00:01:05,110 --> 00:01:11,700
So here's the general idea, and
we use some notations here, so.

13
00:01:11,700 --> 00:01:16,570
X sub i j denotes the rating
of object o j by user u i.

14
00:01:17,820 --> 00:01:23,490
And n sub i is average rating
of all objects by this user.

15
00:01:26,150 --> 00:01:30,310
So this n i is needed.

16
00:01:30,310 --> 00:01:35,500
Because we would like to normalize
the ratings of objects by this user.

17
00:01:35,500 --> 00:01:39,180
So how do you do normalization?

18
00:01:39,180 --> 00:01:40,780
Well, where do you adjust that?

19
00:01:43,330 --> 00:01:46,770
Subtract the,
the average rating from all the ratings.

20
00:01:47,850 --> 00:01:49,890
Now this is the normalized ratings so

21
00:01:49,890 --> 00:01:53,580
that the ratings from different
users will be comparable.

22
00:01:55,590 --> 00:02:01,920
Because some users might be more generous
and they generally give more high ratings.

23
00:02:01,920 --> 00:02:04,160
But, some others might be more critical.

24
00:02:04,160 --> 00:02:08,280
So, their ratings can not be
directly compared with each other or

25
00:02:08,280 --> 00:02:10,820
aggregated them together.

26
00:02:10,820 --> 00:02:13,490
So, we need to do this normalization.

27
00:02:13,490 --> 00:02:17,460
Now, the prediction of the rating.

28
00:02:17,460 --> 00:02:22,770
On the item by another user or
active user, u sub a here

29
00:02:24,470 --> 00:02:29,419
can be based on the average
ratings of similar users.

30
00:02:30,630 --> 00:02:36,950
So the user u sub a is the user that we
are interested in recommending items to.

31
00:02:36,950 --> 00:02:42,410
And we now are interested in
recommending this o sub j.

32
00:02:42,410 --> 00:02:47,920
So we're interested in knowing how
likely this user will like this object.

33
00:02:47,920 --> 00:02:50,380
How do we know that?

34
00:02:50,380 --> 00:02:55,560
Well the idea here is to look at the how
whether similar users to this user

35
00:02:55,560 --> 00:02:57,260
have liked this object.

36
00:02:59,500 --> 00:03:01,830
So mathematically, this is, as you say,

37
00:03:01,830 --> 00:03:07,560
the predict the rating of this
user on this app, object.

38
00:03:07,560 --> 00:03:13,520
User A on object Oj is
basically combination of

39
00:03:13,520 --> 00:03:18,650
the normalized ratings of different users.

40
00:03:18,650 --> 00:03:23,950
And in fact, here,
we're picking a sum of all the users.

41
00:03:23,950 --> 00:03:29,180
But not all users contribute
equally to the average.

42
00:03:29,180 --> 00:03:32,378
And this is controlled by the weights.

43
00:03:32,378 --> 00:03:33,336
So this.

44
00:03:34,587 --> 00:03:40,350
Weight controls the inference
of a user on the prediction.

45
00:03:41,860 --> 00:03:47,110
And of course, naturally this weight
should be related to the similarity

46
00:03:47,110 --> 00:03:51,630
between ua and this particular user, ui.

47
00:03:51,630 --> 00:03:57,650
The more similar they are then
the more contribution we would like

48
00:03:57,650 --> 00:04:02,420
user u i to make in predicting
the preference of u a.

49
00:04:03,950 --> 00:04:05,530
So the formula is extremely simple.

50
00:04:05,530 --> 00:04:10,170
You're going to see it's a sum
of all the possible users.

51
00:04:10,170 --> 00:04:13,880
And inside the sum, we have their ratings,

52
00:04:13,880 --> 00:04:17,380
well their normalized
ratings as I just explained.

53
00:04:17,380 --> 00:04:21,400
The ratings need to be normalized in
order to be comfortable with each other.

54
00:04:22,690 --> 00:04:26,730
And then these ratings
are rated by their similarity.

55
00:04:26,730 --> 00:04:33,280
So we can imagine a W of A and
I is just a similarity of user A user I.

56
00:04:34,470 --> 00:04:35,350
Now, what's k here?

57
00:04:35,350 --> 00:04:39,110
Well, k is a simpler normalizer.

58
00:04:39,110 --> 00:04:45,670
It's just it's just one over the sum
of all the weights, over all the users.

59
00:04:47,780 --> 00:04:53,360
And so this means, basically, if you
consider the weight here together with k.

60
00:04:53,360 --> 00:04:59,320
And we have coefficients or weights
that would sum to one for all the users.

61
00:04:59,320 --> 00:05:05,690
And it's just a normalization strategy,
so that you get this predicted rating

62
00:05:05,690 --> 00:05:12,319
in the same range as the these ratings
that we use to make the prediction.

63
00:05:13,640 --> 00:05:14,560
Right?

64
00:05:14,560 --> 00:05:20,320
So, this is basically the main idea
of memory-based approaches for

65
00:05:20,320 --> 00:05:21,360
collaborative filtering, okay?

66
00:05:22,740 --> 00:05:29,365
Once we make this prediction,
we also would like to map back

67
00:05:29,365 --> 00:05:34,630
to the rating that the user.

68
00:05:34,630 --> 00:05:38,520
The user would actually make.

69
00:05:38,520 --> 00:05:44,110
And this is to further add the,
mean rating or

70
00:05:44,110 --> 00:05:49,950
average rating of this user u
sub a to the predicted value.

71
00:05:49,950 --> 00:05:51,710
This would recover.

72
00:05:51,710 --> 00:05:54,280
A meaningful rating for this user.

73
00:05:54,280 --> 00:05:59,410
So if this user is generous,
then the average would be somewhat high,

74
00:05:59,410 --> 00:06:04,580
and when we added that, the rating will
be adjusted to a relatively high rating.

75
00:06:04,580 --> 00:06:07,110
Now, when you recommend an item to a user,

76
00:06:07,110 --> 00:06:12,200
this actually doesn't really matter
because you are interested in basically

77
00:06:12,200 --> 00:06:16,210
the normalized rating
that's more meaningful.

78
00:06:17,280 --> 00:06:22,410
But when they evaluate these collaborative
filtering approach is typically

79
00:06:22,410 --> 00:06:31,390
assumed that actual ratings of user
on these objects to be unknown.

80
00:06:31,390 --> 00:06:33,160
And then you do the prediction and

81
00:06:33,160 --> 00:06:39,060
then you compare the predicted
ratings with their actual ratings.

82
00:06:39,060 --> 00:06:42,030
So they,
you do have access to the actual ratings.

83
00:06:42,030 --> 00:06:44,000
But then you pretend you don't know.

84
00:06:44,000 --> 00:06:48,420
And then you compare real systems
predictions with the actual ratings.

85
00:06:48,420 --> 00:06:53,580
In that case, obviously the system's
prediction would have to be adjusted to

86
00:06:53,580 --> 00:06:59,540
match the actual result the user, and this
is not what's happening here, basically.

87
00:07:01,030 --> 00:07:01,750
Okay?

88
00:07:01,750 --> 00:07:05,160
So this is the memory-based approach.

89
00:07:05,160 --> 00:07:06,970
Now of course if you look at the formula,

90
00:07:06,970 --> 00:07:09,430
if you want to write
the program to implement it.

91
00:07:09,430 --> 00:07:15,510
You still face the problem of determining
what is this w function, right?

92
00:07:15,510 --> 00:07:21,020
Once you know the w function, then
the formula is very easy to implement.

93
00:07:22,730 --> 00:07:28,950
So indeed there are many different ways to
compute this function or this weight, w.

94
00:07:28,950 --> 00:07:33,680
And, specific approaches generally
differ in how this is computed.

95
00:07:35,500 --> 00:07:37,880
So, here are some possibilities.

96
00:07:37,880 --> 00:07:42,010
And, you can imagine,
there are many pro, other possibilities.

97
00:07:42,010 --> 00:07:46,460
One popular approach is we use
the Pearson Correlation Coefficient.

98
00:07:48,140 --> 00:07:53,450
This would be a sum of a common
range of items, and the formula

99
00:07:53,450 --> 00:07:58,690
is a standard Pearson correlation
coefficient formula, as shown here.

100
00:07:59,840 --> 00:08:05,367
So, this basically measures
weather the two users tended

101
00:08:05,367 --> 00:08:11,545
to all give higher ratings to similar
items, or lower ratings to similar items.

102
00:08:11,545 --> 00:08:17,806
Another measure is the cosine measure and
this is the retreat the rating vectors as

103
00:08:17,806 --> 00:08:23,808
vectors in the vector space, and then
we're going to measure the the angel and

104
00:08:23,808 --> 00:08:27,870
compute the cosign of
the angle of the two vectors.

105
00:08:27,870 --> 00:08:32,580
And this measure has been used in the
vector space more for retrieval as well.

106
00:08:32,580 --> 00:08:36,400
So as you can imagine, there are so
many different ways of doing that.

107
00:08:36,400 --> 00:08:41,300
In all these cases, note that the user
similarity is based on their preferences

108
00:08:41,300 --> 00:08:47,135
on items, and we did not actually use
any content information of these items.

109
00:08:47,135 --> 00:08:49,671
It didn't matter what these items are.

110
00:08:49,671 --> 00:08:52,235
They can be movies, they can be books,

111
00:08:52,235 --> 00:08:55,395
they can be products,
they can be tax documents.

112
00:08:55,395 --> 00:08:57,930
We just didn't care about the content.

113
00:08:57,930 --> 00:09:07,120
And so this allows such approach to be
applied to a wide range of problems.

114
00:09:07,120 --> 00:09:08,920
Now in some newer approaches of course,

115
00:09:08,920 --> 00:09:11,830
we would like to use more
information about the user.

116
00:09:11,830 --> 00:09:18,720
Clearly, we know more about the user, not
just a, these preferences on these items.

117
00:09:18,720 --> 00:09:24,960
And so in a actual filtering system, using
collaborative filtering, we could also

118
00:09:24,960 --> 00:09:30,100
combine that with content-based filtering,
we could use context information.

119
00:09:30,100 --> 00:09:36,550
And those are all interesting approaches
that people are still studying.

120
00:09:36,550 --> 00:09:39,130
There are newer approaches proposed.

121
00:09:39,130 --> 00:09:43,680
But this approach has been shown
to work reasonably well and

122
00:09:43,680 --> 00:09:45,590
it's easy to implement.

123
00:09:45,590 --> 00:09:49,918
And practical applications
could be a starting point to

124
00:09:49,918 --> 00:09:53,628
see if the strand here works well for
your application.

125
00:09:56,128 --> 00:10:01,470
So there are some obvious ways
to also improve this approach.

126
00:10:01,470 --> 00:10:07,070
And mainly would like to improve
the user similarity measure.

127
00:10:07,070 --> 00:10:09,690
And there are some practical
issues to deal with here as well.

128
00:10:09,690 --> 00:10:11,960
So for example,
there will be a lot of missing values.

129
00:10:11,960 --> 00:10:12,970
What do you do with them?

130
00:10:12,970 --> 00:10:18,050
Well, you can set them to default values
or the average ratings of the user.

131
00:10:18,050 --> 00:10:20,310
And that will be a simple solution.

132
00:10:20,310 --> 00:10:26,400
But there are advantages to approaches
that can actually try to predict those

133
00:10:26,400 --> 00:10:33,010
missing values and then use the predicted
values to improve the similarity.

134
00:10:33,010 --> 00:10:35,590
So in fact, the memory database approach,

135
00:10:35,590 --> 00:10:39,105
you can predict those with missing values,
right?

136
00:10:39,105 --> 00:10:39,785
So you can imagine,

137
00:10:39,785 --> 00:10:44,075
you have iterative approach where you
first do some preliminary prediction and

138
00:10:44,075 --> 00:10:48,835
then you can use the predictor values to
further improve the similarity function.

139
00:10:48,835 --> 00:10:54,840
Right so this is here is
a way to solve the problem.

140
00:10:54,840 --> 00:10:58,670
And the strategy of this in the effect
of the performance of clarity filtering,

141
00:11:00,320 --> 00:11:05,039
just like in the other heuristics,
we improve the similarity function.

142
00:11:06,300 --> 00:11:10,460
Another idea which is actually very
similar to the idea of IDF that we

143
00:11:10,460 --> 00:11:15,180
have seen in text research, is called
the inverse user frequency or IUF.

144
00:11:15,180 --> 00:11:23,169
Now here the idea is to look at the where
the two users share similar ratings.

145
00:11:24,550 --> 00:11:30,020
If the item is a popular item that has
been aah, viewed by many people and

146
00:11:30,020 --> 00:11:35,770
seemingly leads to people interested
in this item may not be so interesting.

147
00:11:36,780 --> 00:11:40,610
But if it's a rare item and
has not been viewed by many users.

148
00:11:40,610 --> 00:11:42,878
But, these two users
[INAUDIBLE] to this item.

149
00:11:42,878 --> 00:11:47,336
And they give similar ratings, and it
says more about their similarity, right?

150
00:11:47,336 --> 00:11:52,136
So it's kind of to emphasize
more on similarity

151
00:11:52,136 --> 00:11:56,461
on items that are not
viewed by many users.

152
00:11:56,461 --> 00:12:06,461
[MUSIC]

